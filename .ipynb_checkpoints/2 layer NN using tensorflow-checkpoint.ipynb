{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_folders = ['/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Train5.5/pos',\n",
    "                 '/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Train5.5/neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_folders = ['/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Test5.5/pos', \n",
    "                '/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Test5.5/neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Train5.5/pos.pickle already present - Skipping pickling.\n",
      "/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Train5.5/neg.pickle already present - Skipping pickling.\n",
      "/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Test5.5/pos.pickle already present - Skipping pickling.\n",
      "/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/Test5.5/neg.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "image_width = 24\n",
    "image_height = 40\n",
    "\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_class(folder, min_num_images, im_height, im_width):\n",
    "\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), im_height, im_width), dtype=np.float32)\n",
    "  \n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            #Image.open('image.png').convert('LA')\n",
    "            image_data = (rgb2gray(ndimage.imread(image_file)).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            #image_data = ((Image.open(image_file).convert('LA')).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (im_height, im_width):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, im_height, im_width, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_class(folder, min_num_images_per_class, im_height, im_width)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 90, image_height, image_width)\n",
    "test_datasets = maybe_pickle(test_folders, 90, image_height, image_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2d77a31590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAAD8CAYAAADjVO9VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSOzvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d77b42c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = pickle.load(open(train_datasets[0]))\n",
    "plt.imshow(a[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (500, 40, 24) (500,)\n",
      "Validation: (98, 40, 24) (98,)\n",
      "Testing: (216, 40, 24) (216,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_h, img_w):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_h, img_w), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "  #print (num_classes)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_height, image_width)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_height, image_width)\n",
    "    vsize_per_class = valid_size // num_classes # 49\n",
    "    tsize_per_class = train_size // num_classes # 250\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class #299\n",
    "    #print (vsize_per_class)\n",
    "    #print (tsize_per_class)\n",
    "    #print (end_l)\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                #print (label)\n",
    "        \n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    #print (vsize_per_class)\n",
    "                    #print (end_l)\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                #print (vsize_per_class)\n",
    "                #print (end_l)\n",
    "                \n",
    "                #print (len(valid_dataset))\n",
    "                #print (len(train_letter))\n",
    "                #print (len(train_dataset))\n",
    "\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                #print (start_t)\n",
    "                #print (end_t)\n",
    "                train_labels[start_t:end_t] = label\n",
    "                #print (train_labels)\n",
    "                #print (valid_labels)\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    #print (train_labels)\n",
    "    #print (valid_labels)\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 500\n",
    "valid_size = 98\n",
    "test_size = 216\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/complete_dataset.pickle\n"
     ]
    }
   ],
   "source": [
    "pickle_file = os.path.join('/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/', 'complete_dataset.pickle')\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print (pickle_file)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 3129449\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (500, 40, 24) (500,)\n",
      "Validation set (98, 40, 24) (98,)\n",
      "Test set (216, 40, 24) (216,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/media/sumit/New Volume1/Project/From Henry/downsampled_image_by4/complete_dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score: 0.842593\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "train_dataset_temp = train_dataset\n",
    "nsamples, nx, ny = train_dataset_temp.shape\n",
    "X_train = train_dataset_temp.reshape((nsamples,nx*ny))\n",
    "\n",
    "test_dataset_temp = test_dataset\n",
    "nsamples, nx, ny = test_dataset_temp.shape\n",
    "X_test = test_dataset_temp.reshape((nsamples,nx*ny))\n",
    "\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (500, 960) (500, 2)\n",
      "Validation set (98, 960) (98, 2)\n",
      "Test set (216, 960) (216, 2)\n"
     ]
    }
   ],
   "source": [
    "image_width = 24\n",
    "image_height = 40\n",
    "num_labels = 2\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_height * image_width)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    HIDDEN_UNITS = 100\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_height * image_width, HIDDEN_UNITS]))\n",
    "    biases_1 = tf.Variable(tf.zeros([HIDDEN_UNITS]))\n",
    "    \n",
    "    layer_1_outputs = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    \n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([HIDDEN_UNITS, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    logits = tf.nn.sigmoid(tf.matmul(layer_1_outputs, weights_2) + biases_2)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "    test_prediction = tf.nn.sigmoid(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 0.693794\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 68.4%\n",
      "Loss at step 100: 0.442518\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 83.7%\n",
      "Loss at step 200: 0.418450\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 84.7%\n",
      "Loss at step 300: 0.409484\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 84.7%\n",
      "Loss at step 400: 0.406789\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 84.7%\n",
      "Loss at step 500: 0.401739\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 84.7%\n",
      "Loss at step 600: 0.399564\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 700: 0.397750\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 800: 0.396201\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 900: 0.395838\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 1000: 0.394114\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 86.7%\n",
      "Loss at step 1100: 0.388311\n",
      "Training accuracy: 91.6%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 1200: 0.386497\n",
      "Training accuracy: 91.6%\n",
      "Validation accuracy: 84.7%\n",
      "Loss at step 1300: 0.386159\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 85.7%\n",
      "Loss at step 1400: 0.384690\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 85.7%\n",
      "Test accuracy: 84.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1401\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
